{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e970ba",
   "metadata": {},
   "source": [
    "# Tutorial: Criando um chat bot simples com a API Gemini\n",
    "\n",
    "Este notebook guia voce passo a passo na construcao de um chat bot que utiliza a plataforma Gemini da Google para responder perguntas em portugues. A proposta e que voce entenda cada etapa - da instalacao a criacao de um laco interativo - e adapte o codigo as suas necessidades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a24b27",
   "metadata": {},
   "source": [
    "## Visao geral\n",
    "\n",
    "Ao final, voce tera:\n",
    "- uma configuracao segura da sua chave de API;\n",
    "- um cliente da API pronto para enviar perguntas isoladas;\n",
    "- um chat com memoria contextual;\n",
    "- um laco interativo completo, incluindo historico das mensagens.\n",
    "\n",
    "Se preferir executar em outro ambiente (VS Code, Colab, etc.), os passos permanecem os mesmos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e449fc",
   "metadata": {},
   "source": [
    "## Pre-requisitos\n",
    "\n",
    "- Python 3.10 ou superior.\n",
    "- Uma conta no Google AI Studio com acesso ao modelo Gemini.\n",
    "- Uma chave de API ativa e com permissoes para o modelo `gemini-2.5-flash`.\n",
    "- Biblioteca `google-genai` instalada (mostramos a instalacao no proximo passo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26bdcc6",
   "metadata": {},
   "source": [
    "## 1. Instalar dependencias\n",
    "\n",
    "Se voce estiver em um notebook (Jupyter, Colab, etc.), execute a celula abaixo para instalar ou atualizar a biblioteca `google-genai`. Em ambientes gerenciados, lembre-se de verificar as politicas de instalacao antes de rodar comandos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ee638",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e29700",
   "metadata": {},
   "source": [
    "## 2. Configurar a sua chave de API\n",
    "\n",
    "Armazene a chave em uma variavel de ambiente para evitar expor o valor diretamente no codigo. No notebook, voce pode defini-la manualmente (apenas para testes) ou usar cofres/segredos da plataforma.\n",
    "\n",
    "> Dica: Se estiver no Google Colab, substitua a leitura da variavel por `from google.colab import userdata` e recupere o segredo com `userdata.get(\"NOME_DO_SEGREDO\")`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "API_KEY = os.getenv('GOOGLE_API_KEY', 'COLE_SUA_CHAVE_AQUI')\n",
    "if API_KEY == 'COLE_SUA_CHAVE_AQUI':\n",
    "    raise ValueError(\n",
    "        \"Defina a variavel de ambiente GOOGLE_API_KEY ou substitua 'COLE_SUA_CHAVE_AQUI' pelo valor real da sua chave.\"\n",
    "    )\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = API_KEY\n",
    "print('Chave configurada com sucesso.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e41a9",
   "metadata": {},
   "source": [
    "## 3. Conectar ao modelo Gemini\n",
    "\n",
    "Com a chave disponivel, criamos um cliente para conversar com o modelo. Execute esta etapa apenas uma vez por sessao.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "print('Cliente inicializado com sucesso.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57f66a",
   "metadata": {},
   "source": [
    "## 4. Primeiro teste: pergunta isolada\n",
    "\n",
    "Use `generate_content` para enviar perguntas pontuais. E uma forma rapida de validar credenciais e latencia antes de partir para o modo de chat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bf147",
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = input('Digite uma pergunta rapida para o modelo: ')\n",
    "resposta = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents=pergunta,\n",
    ")\n",
    "print('Resposta:\\n', resposta.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338c3e1",
   "metadata": {},
   "source": [
    "## 5. Iniciar um chat com memoria contextual\n",
    "\n",
    "Crie uma sessao de chat persistente para que o modelo considere o historico de mensagens antes de responder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471453c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(model='gemini-2.5-flash')\n",
    "print('Chat iniciado. Faca uma pergunta inicial para abrir a conversa.')\n",
    "primeira_pergunta = input('Voce: ')\n",
    "primeira_resposta = chat.send_message(primeira_pergunta)\n",
    "print('Gemini:\\n', primeira_resposta.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60cc409",
   "metadata": {},
   "source": [
    "## 6. Construir um laco interativo\n",
    "\n",
    "A funcao abaixo mantem o dialogo aberto ate voce digitar `fim`. Ela tambem ignora entradas vazias para evitar chamadas desnecessarias a API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2bd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversar(chat_session):\n",
    "    print(\"\\nDigite 'fim' para encerrar a conversa.\\n\")\n",
    "    while True:\n",
    "        mensagem = input('Voce: ').strip()\n",
    "        if not mensagem:\n",
    "            print('Mensagem vazia ignorada.')\n",
    "            continue\n",
    "        if mensagem.lower() == 'fim':\n",
    "            print('Encerrando a conversa.')\n",
    "            break\n",
    "\n",
    "        resposta = chat_session.send_message(mensagem)\n",
    "        print('Gemini:\\n', resposta.text, '\\n')\n",
    "\n",
    "conversar(chat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055f56a",
   "metadata": {},
   "source": [
    "## 7. Consultar o historico de mensagens\n",
    "\n",
    "Depois da conversa, recupere o historico completo para salvar logs, gerar analises ou alimentar outras ferramentas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d733e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historico = chat.get_history()\n",
    "\n",
    "for indice, turno in enumerate(historico, start=1):\n",
    "    papel = turno.role.capitalize()\n",
    "    partes = getattr(turno, 'parts', [])\n",
    "    conteudo = '\n",
    "'.join(\n",
    "        getattr(parte, 'text', '')\n",
    "        for parte in partes\n",
    "        if getattr(parte, 'text', '')\n",
    "    ) or '[mensagem vazia]'\n",
    "    print(f\"{indice}. {papel}: {conteudo}\n",
    "\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8673b2",
   "metadata": {},
   "source": [
    "## Proximos passos\n",
    "\n",
    "- Troque o modelo para variantes mais poderosas (`gemini-2.0-pro`, etc.) conforme a necessidade.\n",
    "- Trate erros e limites de taxa antes de levar o chat para producao.\n",
    "- Conecte este backend a uma interface (web, CLI, WhatsApp) reaproveitando as funcoes apresentadas aqui.\n",
    "\n",
    "Bom estudo!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
